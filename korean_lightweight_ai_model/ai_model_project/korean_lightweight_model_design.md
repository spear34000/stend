# 한국어 특화 경량화 AI 모델 설계

## 1. 모델 설계 개요

### 1.1 목표 및 요구사항
- **하드웨어 제약**: RAM 3GB, SSD 64GB 환경에서 구동 가능
- **언어 특화**: 한국어에 최적화된 성능 제공
- **용도**: 일반적인 한국어 텍스트 생성 및 이해

### 1.2 설계 접근 방식
- **경량 아키텍처 선택**: 작은 크기의 기본 모델 사용
- **극단적 최적화**: 다양한 최적화 기법 복합 적용
- **한국어 특화**: 한국어 데이터와 토크나이저 최적화

## 2. 기본 모델 선택

### 2.1 모델 크기 및 아키텍처
- **기본 모델**: Polyglot-ko 410M
- **아키텍처**: Transformer 기반 디코더 전용 모델
- **파라미터 수**: 410M (약 4억 1천만 개)
- **선택 이유**:
  - 한국어에 특화된 사전 학습 모델
  - 상대적으로 작은 크기로 최적화 용이
  - 한국어 이해 능력이 검증됨

### 2.2 토크나이저 설계
- **한국어 최적화 토크나이저**: 한국어 형태소 기반 토크나이저 사용
- **어휘 사전 크기**: 32,000 토큰으로 제한
- **특수 토큰**: 한국어 특화 특수 토큰 추가
- **최적화 방향**: 한국어 토큰화 효율성 향상으로 컨텍스트 길이 최적화

## 3. 모델 경량화 전략

### 3.1 양자화 적용
- **양자화 수준**: INT4 (4비트) 양자화 적용
- **양자화 방식**: 동적 범위 양자화(Dynamic Range Quantization)
- **예상 효과**: 
  - 410M 모델 × INT4 양자화 = 약 205MB (모델 가중치)
  - FP16 대비 약 75% 메모리 절감

### 3.2 모델 구조 최적화
- **어텐션 메커니즘**: 플래시 어텐션(Flash Attention) 적용
- **헤드 구성**: 그룹 쿼리 어텐션(GQA) 적용, 8개 헤드를 2개 그룹으로 구성
- **레이어 수 최적화**: 12개 레이어에서 8개 레이어로 감소
- **임베딩 차원 최적화**: 임베딩 차원을 1024에서 768로 감소

### 3.3 프루닝 적용
- **구조적 프루닝**: 중요도가 낮은 어텐션 헤드 제거
- **가중치 프루닝**: 임계값 이하의 가중치를 0으로 설정
- **희소성 목표**: 50% 이상의 가중치를 0으로 만들어 메모리 사용량 감소

## 4. 추론 최적화 전략

### 4.1 KV 캐시 최적화
- **캐시 크기 제한**: 최대 512 토큰으로 제한
- **캐시 압축**: 8비트 KV 캐시 사용
- **캐시 관리**: 슬라이딩 윈도우 방식으로 오래된 캐시 제거

### 4.2 메모리 관리
- **그래디언트 체크포인팅**: 활성화 값 재계산으로 메모리 사용량 감소
- **메모리 효율적 연산**: 인플레이스 연산(in-place operations) 최대화
- **배치 처리 최적화**: 배치 크기 1로 고정하여 메모리 사용량 최소화

### 4.3 디스크 활용 전략
- **모델 샤딩**: 모델을 여러 부분으로 나누어 필요할 때만 메모리에 로드
- **메모리 매핑**: 메모리 매핑 기법을 사용하여 디스크와 RAM 간 효율적 데이터 교환
- **스왑 최적화**: SSD를 스왑 공간으로 효율적으로 활용

## 5. 한국어 특화 최적화

### 5.1 한국어 데이터 최적화
- **한국어 코퍼스 선별**: 고품질 한국어 데이터 중심으로 학습
- **도메인 특화**: 일상 대화 및 정보 검색 중심의 데이터 선별
- **데이터 필터링**: 불필요한 영어 및 기타 언어 데이터 제거

### 5.2 한국어 토큰화 효율성
- **형태소 분석 기반**: 한국어 형태소 분석을 통한 효율적 토큰화
- **자주 사용되는 한국어 표현**: 자주 사용되는 한국어 표현을 단일 토큰으로 처리
- **한글 자모 분리 방지**: 한글 자모 분리를 최소화하여 토큰 수 감소

### 5.3 한국어 특화 학습 전략
- **한국어 특화 사전 학습**: 한국어 데이터에 대한 추가 사전 학습
- **한국어 지시어 튜닝**: 한국어 지시어 데이터셋으로 미세 조정
- **한국어 문법 및 맥락 강화**: 한국어 문법 및 맥락 이해를 위한 특수 학습 태스크

## 6. 구현 계획

### 6.1 개발 환경
- **프레임워크**: PyTorch + Transformers 라이브러리
- **최적화 도구**: ONNX Runtime, TensorRT
- **양자화 도구**: bitsandbytes, GPTQ

### 6.2 구현 단계
1. **기본 모델 준비**: Polyglot-ko 410M 모델 다운로드 및 설정
2. **모델 구조 수정**: 레이어 수 및 임베딩 차원 조정
3. **양자화 적용**: INT4 양자화 적용
4. **프루닝 적용**: 구조적 프루닝 및 가중치 프루닝 적용
5. **추론 최적화**: KV 캐시 최적화 및 메모리 관리 구현
6. **한국어 특화 조정**: 한국어 데이터로 추가 학습 및 미세 조정

### 6.3 평가 방법
- **메모리 사용량**: 3GB RAM 환경에서 안정적 구동 여부
- **추론 속도**: 토큰 생성 속도 측정
- **한국어 성능**: 한국어 이해 및 생성 능력 평가
- **벤치마크**: KOBEST, KoBEST-MT 등 한국어 벤치마크 활용

## 7. 예상 성능 및 한계

### 7.1 예상 성능
- **메모리 사용량**: 최대 2.5GB (안전 마진 0.5GB)
- **추론 속도**: 초당 2-5 토큰 생성
- **한국어 성능**: 기존 1B 이상 모델의 70-80% 수준 성능 유지

### 7.2 예상 한계 및 대응 방안
- **컨텍스트 길이 제한**: 최대 512 토큰으로 제한될 수 있음
- **복잡한 추론 능력 감소**: 수학적 추론, 복잡한 논리 추론 능력 제한
- **대응 방안**: 
  - 특정 도메인에 특화된 버전 개발
  - 외부 지식 베이스 연동으로 한계 보완
  - 점진적 응답 생성으로 컨텍스트 길이 제한 극복

## 8. 결론

본 설계는 RAM 3GB, SSD 64GB의 제한된 환경에서 구동 가능한 한국어 특화 경량 AI 모델을 구현하기 위한 종합적인 접근 방식을 제시합니다. Polyglot-ko 410M을 기반으로 하여 극단적인 양자화, 구조 최적화, 프루닝, 추론 최적화, 한국어 특화 기법을 복합적으로 적용함으로써, 제한된 하드웨어에서도 실용적인 수준의 한국어 AI 모델을 구현할 수 있을 것으로 기대됩니다.
