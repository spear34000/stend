# 경량화 및 초고효율 한국어 AI 모델 연구 요약

## 1. 최신 AI 모델 트렌드

### 경량화 AI 모델 트렌드
- 온디바이스 AI가 주요 트렌드로 부상 중 (2025년)
- 클리카(Clika)의 ACE 툴킷: AI 모델 크기 최대 87% 축소, 12배 빠른 추론 속도, 80% 비용 절감
- 모델 증류(Model Distillation) 기술이 새로운 AI 트렌드로 부상
- 거대 모델의 지식을 작은 모델로 전이하는 기술 발전

### 한국어 특화 모델
- Llama3 Korean Bllossom 8B: 서울과학기술대학교 연구팀 개발, 한국어에 최적화
- 한국어 어휘 3만개 이상 확장, Llama3 대비 25% 더 긴 한국어 컨텍스트 처리 가능
- Q4_K_M 양자화 버전 크기: 4.92GB (RAM 요구사항: 약 6GB)

## 2. 모델 경량화 기법

### 지식 증류(Knowledge Distillation)
- 큰 모델(Teacher)의 지식을 작은 모델(Student)로 전달하는 기법
- Temperature 파라미터를 통해 소프트 타겟 생성 및 조정
- 작은 모델이 큰 모델의 출력 분포를 모방하도록 학습

### 프루닝(Pruning)
- 모델의 불필요한 파라미터나 연결을 제거하여 크기 축소
- 중요도가 낮은 뉴런, 헤드, 임베딩 채널 등을 제거
- NVIDIA의 Nemotron-4 15B 모델을 8B와 4B로 경량화한 사례 존재

### 양자화(Quantization)
- 고정밀도 부동소수점(FP16/FP32)에서 낮은 비트수(INT8/INT4)로 변환
- 메모리 사용량 감소 및 연산 속도 향상
- Q4_K_M, Q2_K 등 다양한 양자화 수준 존재

## 3. 저사양 하드웨어 최적화

### 하드웨어 요구사항
- 목표 환경: RAM 3GB, SSD 64GB
- 현재 경량화된 모델(Llama3 Korean Bllossom 8B Q4_K_M)도 약 4.67GB RAM 필요
- 더 공격적인 경량화 기법 적용 필요

### 추가 최적화 방안
- 더 낮은 비트 양자화(Q2_K) 적용 가능성 검토
- 모델 아키텍처 자체 축소 (레이어 수 감소)
- 어휘 사전(Vocabulary) 크기 최적화
- 추론 시 메모리 효율적인 기법 적용 (Attention 최적화)

## 4. 결론 및 다음 단계

현재 시중에 있는 한국어 특화 모델들은 대부분 3GB RAM 환경에서 구동하기에는 여전히 큰 상태입니다. 따라서 다음과 같은 접근이 필요합니다:

1. 기존 한국어 특화 소형 모델(예: Llama3 Korean Bllossom 8B) 기반으로 시작
2. 프루닝, 지식 증류, 양자화 기법을 복합적으로 적용
3. 한국어 특화 어휘 최적화를 통한 추가 경량화
4. 3GB RAM 환경에 맞는 추론 최적화 기법 개발

다음 단계에서는 한국어 특화 경량 모델의 구체적인 아키텍처와 최적화 방법을 분석하겠습니다.
