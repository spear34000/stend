# 제한된 하드웨어(RAM 3GB, SSD 64GB)를 위한 효율성 최적화 기술 연구

## 1. 메모리 최적화 기법

### 1.1 낮은 정밀도 양자화(Low Precision Quantization)
- **개념**: 모델 가중치를 고정밀도 부동소수점(FP32/FP16)에서 낮은 비트수(INT8/INT4/INT2)로 변환
- **효과**: 
  - FP32(32비트) → 10억 파라미터 모델 = 4GB 메모리 필요
  - FP16/BF16(16비트) → 10억 파라미터 모델 = 2GB 메모리 필요
  - INT8(8비트) → 10억 파라미터 모델 = 1GB 메모리 필요
  - INT4(4비트) → 10억 파라미터 모델 = 500MB 메모리 필요
  - INT2(2비트) → 10억 파라미터 모델 = 250MB 메모리 필요
- **극단적 양자화**: BitNet b1.58과 같은 기법은 -1, 0, 1의 세 가지 값만으로 극한의 양자화 구현

### 1.2 모델 구조 최적화
- **플래시 어텐션(Flash Attention)**: 메모리 효율적인 어텐션 알고리즘으로, 시퀀스의 토큰을 작은 블록으로 나눠 계산
- **그룹 쿼리 어텐션(Grouped Query Attention, GQA)**: 여러 헤드를 그룹으로 묶고, 각 그룹마다 하나의 Key와 Value를 공유
- **KV 캐시(Key-Value Cache)**: 이전 토큰들에 대한 Key와 Value 값 텐서를 저장해두고 재사용하여 중복 계산 방지

### 1.3 모델 압축 기법
- **프루닝(Pruning)**: 중요도가 낮은 가중치를 제거하여 모델의 연결 수를 줄이고 계산량 감소
- **지식 증류(Knowledge Distillation)**: 큰 모델(Teacher)의 지식을 작은 모델(Student)로 전달

## 2. 파라미터 효율적 미세 조정(PEFT)

### 2.1 LoRA(Low Rank Adaptation)
- **개념**: 사전 학습된 가중치를 직접 수정하지 않고, 추가적인 저순위 행렬을 도입하여 학습
- **효과**: 
  - 전체 파라미터는 1.3B인데 학습 가능한 파라미터는 1.5MB로 조정 가능
  - 13억 개에서 1.5백만으로 줄어드는 효과 (약 0.12% 수준)
- **장점**: 
  - 기존 모델의 가중치를 동결하고 소수의 추가 파라미터만 학습
  - 메모리 사용량 크게 감소
  - 학습 시간 단축 (약 20% 내외)
- **설정**: 
  - rank r 값이 1부터 64까지 비교 가능, r = 4에서 가장 좋은 성능을 보이는 경우가 많음
  - 특히 Q, K, V, O에 대해서는 간략히 설명하면:
    - Q (Query): 현재 단어가 다른 단어와 얼마나 관련이 있는지를 평가
    - K (Key): Query가 참조하는 대상, 각 Key는 데이터셋 내의 다른 단어들을 대표
    - V (Value): 실제로 전달될 정보를 담고 있음
    - O (Output): Attention 메커니즘의 결과로, 모든 Query에 대한 가중합인 Value들의 집합

## 3. 한국어 특화 모델 사례 분석

### 3.1 ETRI 이글(Eagle) 모델
- **크기**: 3.1B 파라미터
- **메모리 요구사항**: 8GB GPU 메모리
- **특징**: 한국어 데이터 비중이 높아 연산 효율성 향상
- **한계**: 3GB RAM 환경에서 구동 불가능

### 3.2 Polyglot ko 1.3b 모델
- **크기**: 1.3B 파라미터
- **메모리 요구사항**: 기본 4GB 이상 필요
- **최적화 사례**: 
  - LoRA 적용 시 학습 가능 파라미터를 1.5MB로 축소 가능
  - 16GB GPU에서도 충분히 학습 가능

## 4. 3GB RAM 환경을 위한 접근 방향

### 4.1 모델 크기 선택
- **기본 모델 크기**: 500M 이하의 초소형 모델 선택
- **양자화 수준**: INT4 또는 INT2 수준의 극단적 양자화 적용
- **목표 메모리 사용량**: 
  - 500M 모델 × INT4 양자화 = 250MB (모델 가중치)
  - 활성화 값, KV 캐시 등 고려 시 약 1GB 내외로 예상

### 4.2 추론 최적화 전략
- **플래시 어텐션** 적용으로 메모리 사용량 감소
- **그룹 쿼리 어텐션** 적용으로 KV 캐시 크기 감소
- **컨텍스트 길이 제한**으로 메모리 사용량 조절
- **배치 크기 최소화**로 활성화 값 메모리 사용량 감소

### 4.3 한국어 특화 최적화
- **한국어 특화 토크나이저** 사용으로 토큰 수 감소
- **한국어 필수 어휘만 포함**하는 축소된 어휘 사전 구성
- **도메인 특화** 접근으로 불필요한 지식 제거

## 5. 결론 및 구현 방향

현재 공개된 한국어 특화 모델 중 3GB RAM 환경에서 직접 구동 가능한 모델은 찾기 어렵습니다. 따라서 다음과 같은 접근이 필요합니다:

1. **기반 모델**: 500M 이하의 소형 한국어 모델 선택 (예: Polyglot-ko 410M)
2. **양자화**: INT4/INT2 수준의 극단적 양자화 적용
3. **구조 최적화**: 플래시 어텐션, 그룹 쿼리 어텐션 적용
4. **추론 최적화**: KV 캐시 최적화, 컨텍스트 길이 제한
5. **한국어 특화**: 한국어 특화 토크나이저 및 어휘 사전 최적화

이러한 접근을 통해 3GB RAM 환경에서도 작동 가능한 한국어 특화 경량 모델 구현이 가능할 것으로 예상됩니다.
